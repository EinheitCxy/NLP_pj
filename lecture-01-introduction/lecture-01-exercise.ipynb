{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9640e639-2123-4a75-beeb-8dad3d078832",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">1 Setup</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01ae1b-d50b-421a-a3bb-ae6ba8eaa767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- \n",
    "# Tutorial for Lecture 01, created by Baojian Zhou (bjzhou@fudan.edu.cn)\n",
    "# This file is opened via Jupyter-notebook. To Install it,\n",
    "# please check details in: https://jupyter.org/install\n",
    "# install Jupyter: conda install anaconda::jupyter\n",
    "\n",
    "# Python for beginner (Python & Pycharm)\n",
    "# If you have zero knowledge about Python, no worry. Here is an one hour course: \n",
    "#    https://www.youtube.com/watch?v=kqtD5dpn9C8\n",
    "# You can continue to learn the rest after watching this short-course.\n",
    "# Here is a simple Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0d914-6f58-4f48-8b16-44be490b12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install anaconda::nltk --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f3e88-57e0-4b15-8e4c-529183275305",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::matplotlib --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8575365-dd38-4db6-a282-f79a68988918",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::wordcloud --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0104195-70cc-4eed-9f89-602695ad3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install anaconda::seaborn --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b07438a-23d5-406a-838f-ac22cde0ef4b",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\" id=\"part-1\">Getting Started with NLTK</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff06b4-112c-47db-a06e-d4dc4976f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before import you should install nltk\n",
    "import sys\n",
    "import nltk\n",
    "# download all datasets in nltk\n",
    "nltk.data.path.append('/Users/baojianzhou/nltk_data')\n",
    "nltk.download()\n",
    "# from NLTK's book module, load all items.\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8bc80e-65af-4eaa-ab0a-cc2f308e95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a040b1f-1c8c-426e-b316-254dac3e35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter their names at the Python prompt\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb6551-3f64-4a77-a944-7d9bf2267f6c",
   "metadata": {},
   "source": [
    "![Moby-Dick](figs/moby-dick.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f5677-ab46-4b87-ad11-405d02576c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54372c20-2745-43fe-aff7-dc829999e1a4",
   "metadata": {},
   "source": [
    "![Sense and sensibility](figs/sense-and-sensibility.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a283f22c-6981-4187-bfe3-d35c194b415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent1)\n",
    "print(sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc0a6e2-8c4b-43c4-99e5-b7d35c891a69",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">spaCy </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940669a0-4d8d-4d4d-9689-aa8aa1611cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::spacy --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a77897-66a4-4c0c-99a9-9d0006678e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1691f5-f5a5-437e-8c3b-f0cbbf98b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# Text: The original word text.\n",
    "# Lemma: The base form of the word.\n",
    "# POS: The simple UPOS part-of-speech tag.\n",
    "# Tag: The detailed part-of-speech tag.\n",
    "# Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "# Shape: The word shape â€“ capitalization, punctuation, digits.\n",
    "# is alpha: Is the token an alpha character? (whether it consists only of letters from the alphabet (A-Z or a-z))\n",
    "# is stop: Is the token part of a stop list, i.e. the most common words of the language? \n",
    "#         (A stop list (or stopwords list) is a list of commonly used words in a language that \n",
    "#         are usually ignored during natural language processing (NLP) tasks, such as text analysis or machine learning.)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3d43e-2aca-4010-802a-87ed5d5d86b2",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\" id=\"part-2\">Searching Text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15e95f5-fa3d-4664-ba9c-4405264bea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: plot Cumulative Frequency Plot for 50 Most Frequently Words in Moby Dick\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "font = {'weight' : 'bold', 'size'   : 18}\n",
    "matplotlib.rc('font', **font)\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1daa86-25da-402b-93dc-c907ab93913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: look up the context of word \"monstrous\" in Moby Dick (text1) \n",
    "print('-'*17)\n",
    "text1.concordance(\"monstrous\")\n",
    "print('-'*17)\n",
    "text1.concordance(\"fudan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609fff1b-cc7a-4744-a1b4-ac31e3bacc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: search Sense and Sensibility (text2) for the word \"affection\"\n",
    "print('-'*17)\n",
    "text2.concordance(\"affection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f1ccdd-71d1-4aee-90dc-2017a099e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: search the book of Genesis (text3) to find out how long some people lived\n",
    "text3.concordance(\"lived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037f7ad-1008-4158-aa18-ac97cf2e3e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: look at text4, the Inaugural Address Corpus, to see examples of English going back to 1789, \n",
    "print('-'*17)\n",
    "text4.concordance(\"nation\")\n",
    "print('-'*17)\n",
    "text4.concordance(\"terror\")\n",
    "# see how these words have been used differently over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3677f4e-3260-4a71-9469-64a41613a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find lol context in text5, the NPS Chat Corpus (Corpus consisting of online chatroom conversations): \n",
    "#       search this for unconventional words like im, ur, lol.\n",
    "text5.concordance(\"lol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33bf03-8833-4e62-970d-1eb34e83c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find similar context,\n",
    "#.      we saw that monstrous occurred in contexts such as \n",
    "#.      the ___ pictures and a ___ size. What other words \n",
    "#.      appear in a similar range of contexts?\n",
    "text1.concordance(\"monstrous\")\n",
    "print('-'*17)\n",
    "text1.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85582ef4-1c8f-42db-bae1-d92f24f7c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find similar context of monstrous in text2\n",
    "text2.concordance(\"monstrous\")\n",
    "print('-'*17)\n",
    "text2.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14875e89-5c10-4971-a526-02cf74d1527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c2740-565d-4c36-a9a4-abee37d7f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find common context of two words\n",
    "# The term \"common_contexts\" allows us to examine \n",
    "# just the contexts that are shared by two or more words\n",
    "text1.common_contexts([\"good\", \"great\"])\n",
    "text1.common_contexts([\"monstrous\", \"very\"])\n",
    "\n",
    "text2.common_contexts([\"monstrous\", \"very\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca3050-46dc-4db0-8bf4-96dc4af34649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispersion_plot(text, words, ignore_case=False, title=\"Lexical Dispersion Plot\"):\n",
    "    \"\"\"\n",
    "    Generate a lexical dispersion plot.\n",
    "\n",
    "    :param text: The source text\n",
    "    :type text: list(str) or iter(str)\n",
    "    :param words: The target words\n",
    "    :type words: list of str\n",
    "    :param ignore_case: flag to set if case should be ignored when searching text\n",
    "    :type ignore_case: bool\n",
    "    :return: a matplotlib Axes object that may still be modified before plotting\n",
    "    :rtype: Axes\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"The plot function requires matplotlib to be installed. \"\n",
    "            \"See https://matplotlib.org/\"\n",
    "        ) from e\n",
    "\n",
    "    word2y = {\n",
    "        word.casefold() if ignore_case else word: y\n",
    "        for y, word in enumerate(reversed(words)) # should not be reversed(words)\n",
    "    }\n",
    "    xs, ys = [], []\n",
    "    for x, token in enumerate(text):\n",
    "        token = token.casefold() if ignore_case else token\n",
    "        y = word2y.get(token)\n",
    "        if y is not None:\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    ax.plot(xs, ys, \"|\")\n",
    "    ax.set_yticks(list(range(len(words))), reversed(words), color=\"C0\") # or put revered here.\n",
    "    ax.set_ylim(-1, len(words))\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Word Offset\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7eed0-9f3d-4f72-aeeb-3e5f2efb4f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "words = [\"Elinor\", \"Marianne\", \"Edward\", \"Willoughby\"]\n",
    "dispersion_plot(gutenberg.words(\"austen-sense.txt\"), words)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb073c-8b25-4b0c-9164-cbe2966fce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: generate some random text in the various styles we have just seen.\n",
    "text1.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593f23ad-6273-4e38-ba13-b1142f37ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: generate word cloud you may need to install wordcloud first\n",
    "words = [w for w in text2]\n",
    "wc = WordCloud(background_color='white', max_words=2000, stopwords=STOPWORDS, max_font_size=50, random_state=17)\n",
    "wc.generate(' '.join(words))\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b016aa-e9a7-40d8-92ac-8c2c487b641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: count how many words (including punctuation symbols) in the book of Genesis\n",
    "len(text3)\n",
    "# So Genesis has 44,764 words and punctuation symbols, or \"tokens.\" \n",
    "# A token is the technical name for a sequence of characters â€” \n",
    "# such as hairy, his, or :) â€” that we want to treat as a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4df10-f684-4252-a946-7f2eeb654298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: calculate a measure of the lexical richness of the text. \n",
    "print(f\"{len(set(text3)) / len(text3) * 100:.2f}%\")\n",
    "# the number of distinct words is just 6% of the total number of words, \n",
    "# or equivalently that each word is used 16 times on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c451f12e-dac3-40c9-b6cf-6b97aec25370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: count a specific word\n",
    "print(text3.count(\"the\"))\n",
    "print('-'*17)\n",
    "# count percentage of the word \"a\" used in the book\n",
    "val = 100 * text4.count(r\"a\") / len(text4)\n",
    "print(f\"{val:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5873807-9552-4e0c-89c5-e9700297b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: counting lol\n",
    "print(text1.count(\"lol\"))\n",
    "print(text2.count(\"lol\"))\n",
    "print(text3.count(\"lol\"))\n",
    "print(text4.count(\"lol\"))\n",
    "print(text5.count(\"lol\"))\n",
    "print(text6.count(\"lol\"))\n",
    "# text5 is a corpus of Online Chat Dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be8ac4-f1bd-4975-808e-3175ed050996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick out the first of these texts â€” Emma by Jane Austen â€” \n",
    "# and give it a short name, emma, then  find out how many words \n",
    "# it contains\n",
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4279e36-3be6-4d11-8f3a-ef0284031340",
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
    "emma.concordance(\"surprize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3283106-a22a-43e0-8184-10249a51bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid))\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "    print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a9940-9104-4e57-bbb1-7552eee9dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average word length, average sentence length,\n",
    "# and the number of times each vocabulary item appears \n",
    "# in the text on average (our lexical diversity score). \n",
    "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "print(macbeth_sentences)\n",
    "print(macbeth_sentences[1116])\n",
    "longest_len = max(len(s) for s in macbeth_sentences)\n",
    "print([s for s in macbeth_sentences if len(s) == longest_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a989b1d-5578-4fd0-8261-2e9761549e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: create a frequency distribution for text1 (Moby-Dick text)\n",
    "\n",
    "# A frequency distribution for the outcomes of an experiment. A frequency distribution \n",
    "# records the number of times each outcome of an experiment has occurred. For example, \n",
    "# a frequency distribution could be used to record the frequency of each word type in \n",
    "# a document. \n",
    "fdist1 = FreqDist(text1)\n",
    "print(fdist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f650753-8960-487e-9b14-f0114dd89fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find the 50 most frequent words of text1\n",
    "print(fdist1.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d3463-e23b-4647-bd9e-97e5d3c14ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find the frequency of word 'whale' in text1\n",
    "print(fdist1['whale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f516da-18a2-4d87-9274-c2a33a70bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "font = {'weight' : 'bold', 'size'   : 18}\n",
    "matplotlib.rc('font', **font)\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
    "\n",
    "# Task: plot Probability Density Function\n",
    "fdist1.plot(50, cumulative=False)\n",
    "# What do you find ?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ad724-704a-4a91-af0c-74df15bf6b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analysis = nltk.FreqDist(text1)\n",
    "filter_words = dict([(m,n) for m,n in data_analysis.items() if len(m) > 3])\n",
    "data_analysis = nltk.FreqDist(filter_words)\n",
    "data_analysis.plot(25,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd07413-4ecb-48f9-8d16-4ac6ebc7cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1.plot(50, cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0275f9-76c4-4402-a159-a6fee1397e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find and cluster frequency of word length\n",
    "\n",
    "# For example, we can look at the distribution of word lengths \n",
    "# in a text, by creating a FreqDist out of a long list of numbers, \n",
    "# where each number is the length of the corresponding word in the text:\n",
    "\n",
    "fdist = FreqDist(len(w) for w in text1)\n",
    "print(fdist)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea54720-0d41-42ce-9680-7b6d50357ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: show all frequent of the different lengths of words\n",
    "fdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372afc1-e3b9-4955-a1cd-41ddb39e44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find a specific frequency the most frequency length\n",
    "print(fdist.max())\n",
    "print(fdist[3])\n",
    "print(fdist.freq(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bfc3d5-95d7-4919-abcb-2041dde4125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: what is this ?\n",
    "print(sorted(w for w in set(text1) if w.endswith('ableness'))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76761110-c023-4844-869c-f120fe3d46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: what is this ?\n",
    "print(sorted(term for term in set(text4) if 'gnt' in term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47301d18-10c2-4695-80df-177c289f7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: what is this ?\n",
    "print(sorted(item for item in set(text6) if item.istitle())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c0f55-8d2e-418c-883c-202d458cc32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: what is this ?\n",
    "print(sorted(item for item in set(sent7) if item.isdigit()))\n",
    "print(sorted(w for w in set(text7) if '-' in w and 'index' in w))\n",
    "print(sorted(wd for wd in set(text3) if wd.istitle() and len(wd) > 10))\n",
    "print(sorted(w for w in set(sent7) if not w.islower()))\n",
    "print(sorted(t for t in set(text2) if 'cie' in t or 'cei' in t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b71a7f6-a828-42a9-97b4-14889524ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "captical_words = [w.upper() for w in text1]\n",
    "print(captical_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe448105-9864-492e-8a4b-230810c0ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text1)\n",
    "print(len(text1))\n",
    "print(len(set(text1)))\n",
    "print(len(set(word.lower() for word in text1)))\n",
    "# merge words like The the."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0d046-1d98-4669-8b0e-63d6477c5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate numbers and punctuation from the vocabulary count by \n",
    "# filtering out any non-alphabetic items:\n",
    "print(len(set(word.lower() for word in text1 if word.isalpha())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346fa2fd-aa92-4128-b5de-2c88953c9c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the word type for sent1\n",
    "for token in sent1:\n",
    "    if token.islower():\n",
    "        print(f'{token:10} is a lowercase word')\n",
    "    elif token.istitle():\n",
    "        print(f'{token:10} is a titlecase word')\n",
    "    else:\n",
    "        print(f'{token:10} is punctuation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da95af-4bc0-4ce9-94a0-0268d2307b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of cie and cei words, \n",
    "# then we loop over each item and print it. \n",
    "tricky = sorted(w for w in set(text2) if 'cie' in w or 'cei' in w)\n",
    "for word in tricky:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4aaa4c-e378-418f-8389-ac183365df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web and Chat Text\n",
    "from nltk.corpus import webtext\n",
    "for fileid in webtext.fileids():\n",
    "    print(fileid, webtext.raw(fileid)[:65], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b13944-362c-4cf6-815a-c979f85ba5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: look at the long words of the book Moby-Dick (text1)\n",
    "#       find all words that have at least 15 chars.\n",
    "V = set(text1)\n",
    "long_words = [w for w in V if len(w) > 15]\n",
    "print(sorted(long_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676284c-f683-4937-99d9-69638c5d8c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: look at the long words of the Inaugural Address Corpus (text4)\n",
    "#       find all words that have at least 15 chars.\n",
    "V = set(text4)\n",
    "long_words = [w for w in V if len(w) > 15]\n",
    "print(sorted(long_words))\n",
    "\n",
    "# What do you find ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e2eed-0857-41b8-9966-529bbcdba47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find all long words of text5\n",
    "print(sorted([w for w in set(text5) if len(w) > 15]))\n",
    "\n",
    "# What do you find ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83bfbd5-5270-4be7-91b7-f0a6d93a618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find frequently occurring long words. \n",
    "fdist5 = FreqDist(text5)\n",
    "print(sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7))\n",
    "\n",
    "# What do you find ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f180f-47a4-4991-b6d2-0db40060afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: find hapaxes (hapax legomenon, å­¤ç«‹è¯ï¼Œæ–‡æœ¬ä¸­å‡ºç°ä¸€æ¬¡)\n",
    "fdist1 = FreqDist(text1)\n",
    "hapax = fdist1.hapaxes()\n",
    "print(hapax[:10])\n",
    "\n",
    "# What do you find ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5502b9b-77e2-4805-b50a-5f3f910d210b",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Regular Expression</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1cc0c-1abd-4b84-a5d2-b46de912355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python, there is a built in lib re, we can import them\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8babc-ac49-4e64-8464-a04c29a95968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Find woodchuck or Woodchuck : Disjunction\n",
    "test_str = \"This string contains Woodchuck and woodchuck.\"\n",
    "result=re.search(pattern=\"[wW]oodchuck\", string=test_str)\n",
    "print(result)\n",
    "result=re.search(pattern=r\"[wW]ooodchuck\", string=test_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37b715-4dd3-4b7a-ae0c-ecc1089dfe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the word \"woodchuck\" in the following test string\n",
    "test_str = \"interesting links to woodchucks ! and lemurs!\"\n",
    "re.search(pattern=\"woodchuck\", string=test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb6b155-6bbc-4842-90b1-492a22346d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find !, it follows the same way:\n",
    "print(re.search(pattern=\"!\", string=test_str))\n",
    "print(re.search(pattern=\"!!\", string=test_str))\n",
    "assert re.search(pattern=\"!!\", string=test_str) == None # match nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e1c995-2a79-4aab-a81a-7adaaab3eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find any single digit in a string.\n",
    "result=re.search(pattern=r\"[0123456789]\", string=\"plenty of 7 to 5\")\n",
    "print(result)\n",
    "result=re.search(pattern=r\"[0-9]\", string=\"plenty of 7 to 5\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30ad50-2288-4c02-af8d-6684aa3e4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negation: If the caret ^ is the first symbol after [,\n",
    "# the resulting pattern is negated. For example, the pattern \n",
    "# [^a] matches any single character (including special characters) except a.\n",
    "\n",
    "# -- not an upper case letter\n",
    "print(re.search(pattern=r\"[^A-Z]\", string=\"Oyfn pripetchik\"))\n",
    "\n",
    "# -- neither 'S' nor 's'\n",
    "print(re.search(pattern=r\"[^Ss]\", string=\"I have no exquisite reason for't\"))\n",
    "\n",
    "# -- not a period\n",
    "print(re.search(pattern=r\"[^.]\", string=\"our resident Djinn\"))\n",
    "\n",
    "# -- either 'e' or '^'\n",
    "print(re.search(pattern=r\"[e^]\", string=\"look up ^ now\"))\n",
    "\n",
    "# -- the pattern â€˜a^bâ€™\n",
    "print(re.search(pattern=r'a\\^b', string=r'look up a^b now'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d07799-4334-4ef9-9c02-782fefaa3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More disjuncations\n",
    "str1 = \"Woodchucks is another name for groundhog!\"\n",
    "result = re.search(pattern=\"groundhog|woodchuck\",string=str1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86a5b7-c54a-4ffa-b70f-e2177af10add",
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"Find all woodchuckk Woodchuck Groundhog groundhogxxx!\"\n",
    "result = re.findall(pattern=\"[gG]roundhog|[Ww]oodchuck\",string=str1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b997cc5b-755c-46af-b083-e74d23afc692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some special chars\n",
    "\n",
    "# ?: Optional previous char\n",
    "str1 = \"Find all color colour colouur colouuur colouyr\"\n",
    "result = re.findall(pattern=\"colou?r\",string=str1)\n",
    "print(result)\n",
    "\n",
    "# *: 0 or more of previous char\n",
    "str1 = \"Find all color colour colouur colouuur colouyr\"\n",
    "result = re.findall(pattern=\"colou*r\",string=str1)\n",
    "print(result)\n",
    "\n",
    "# +: 1 or more of previous char\n",
    "str1 = \"baa baaa baaaa baaaaa\"\n",
    "result = re.findall(pattern=\"baa+\",string=str1)\n",
    "print(result)\n",
    "# .: any char\n",
    "str1 = \"begin begun begun beg3n\"\n",
    "result = re.findall(pattern=\"beg.n\",string=str1)\n",
    "print(result)\n",
    "str1 = \"The end.\"\n",
    "result = re.findall(pattern=\"\\.$\",string=str1)\n",
    "print(result)\n",
    "str1 = \"The end? The end. #t\"\n",
    "result = re.findall(pattern=\".$\",string=str1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82cc7d-9ee3-49e6-aa51-eb684f6827c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all \"the\" in a raw text.\n",
    "text = \"If two sequences in an alignment share a common ancestor, \\\n",
    "mismatches can be interpreted as point mutations and gaps as indels (that \\\n",
    "is, insertion or deletion mutations) introduced in one or both lineages in \\\n",
    "the time since they diverged from one another. In sequence alignments of \\\n",
    "proteins, the degree of similarity between amino acids occupying a \\\n",
    "particular position in the sequence can be interpreted as a rough \\\n",
    "measure of how conserved a particular region or sequence motif is \\\n",
    "among lineages. The absence of substitutions, or the presence of \\\n",
    "only very conservative substitutions (that is, the substitution of \\\n",
    "amino acids whose side chains have similar biochemical properties) in \\\n",
    "a particular region of the sequence, suggest [3] that this region has \\\n",
    "structural or functional importance. Although DNA and RNA nucleotide bases \\\n",
    "are more similar to each other than are amino acids, the conservation of \\\n",
    "base pairs can indicate a similar functional or structural role.\"\n",
    "matches = re.findall(\"[^a-zA-Z][tT]he[^a-zA-Z]\", text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e66b2e6-8bda-4a7c-9028-d3b81cb20470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A nicer way is to do the following\n",
    "\n",
    "matches = re.findall(r\"\\b[tT]he\\b\", text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d547f7-8ff0-445a-8dab-95205dbf7832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikilink_re():\n",
    "    \"\"\" This regex is from the following Github:\n",
    "    https://github.com/WikiLinkGraphs/wikidump\n",
    "    \"\"\"\n",
    "    regex_str = r'''(?P<total>(?P<wikilink>\n",
    "        \\[\\[(?P<link>[Ë†\\n\\|\\]\\[\\<\\>\\{\\}]{0,256})(?:\\|(?P<anchor>[Ë†\\[]*?))?\\]\\])\\w*)\\s?'''\n",
    "    return regex.compile(regex_str, regex.VERBOSE | regex.MULTILINE)\n",
    "\n",
    "# Task: Implement the task shown in Slides 50\n",
    "# You may need to\n",
    "# 1. Download a Wikipedia article xml file\n",
    "# 2. Use RE to extract links."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7225fa6c-8f65-434f-a129-9aaea49e2809",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Words and Corpus</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9c9cc1-29ff-4851-bea7-0e0bb55fadf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to download some corpus\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import indian\n",
    "from nltk.corpus import conll2007"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c732a5f-b4ae-4090-90e5-cff756b870ef",
   "metadata": {},
   "source": [
    "### Word types and word instances (tokens)\n",
    "\n",
    "- **Word types** are the number of distinct words in a corpus; if the set of words in the vocabulary is $V$, the number of types is the vocabulary size $|V|$. \n",
    "\n",
    "- **Word instances** are the total number $N$ of running words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f50da-2df1-4919-95f0-1037c3ecb041",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.words())\n",
    "print(f\"total number of tokens in Brown corpus: {len(brown.words())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e7984-563a-483d-af24-868a4987e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in brown.categories():\n",
    "    print(f\"category {cat} has {len(brown.words(categories=cat))} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c348bca-fb16-425f-940f-57eff4a5a937",
   "metadata": {},
   "source": [
    "## Herdanâ€™s Law or Heap's Law\n",
    "\n",
    "- $N$: the number of word instances of corpus\n",
    "- $|V|$: the number of word types\n",
    "\n",
    "The larger the corpora we look at, the more word types we find, and in fact this relationship between $|V|$ and $N$ is called **Herdan's Law** or **Heaps' Law** after its discoverers (in linguistics and information retrieval respectively). Given $k$ and $\\beta$ positive constants, and $0<\\beta<1$, it has the following form\n",
    "\n",
    "$$\n",
    "|V|=k N^\\beta.\n",
    "$$\n",
    "\n",
    "The value of $\\beta$ depends on the corpus size and the genre, but at least for the large corpora, $\\beta$ ranges from .67 to .75. Roughly then we can say that the vocabulary size for a text goes up significantly faster than the square root of its length in words. Let us test it!\n",
    "Check more on [Heap\\'s Law](https://en.wikipedia.org/wiki/Heaps%27_law)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be546e48-e971-4fd7-be05-4135eb24497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import indian\n",
    "from nltk.corpus import conll2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf6dde-35ea-4144-b03c-66183dde0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot\n",
    "import seaborn\n",
    "import numpy as np\n",
    "# Try to center figures.\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf495ee-3a2e-4c12-9f7f-51411baf1525",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len = []\n",
    "type_len = []\n",
    "for words in [gutenberg.words(), indian.words(), conll2007.words()]:\n",
    "    token_len.append(len(words))\n",
    "    type_len.append(len(nltk.FreqDist(w.lower() for w in words)))\n",
    "    print(token_len[-1], type_len[-1])\n",
    "    \n",
    "print(token_len)\n",
    "sorted_ind = np.argsort(token_len)\n",
    "print(sorted_ind)\n",
    "sorted_N = [token_len[_] for _ in sorted_ind]\n",
    "sorted_V = [type_len[_] for _ in sorted_ind]\n",
    "print(sorted_N, sorted_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb6188-84cd-453c-8fc8-3ce1d94f70b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = .7\n",
    "k = 50.\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.plot(sorted_V, [k*(N**beta) for N in sorted(token_len)], c='r',marker=\"D\",linewidth=3., label=\"Herdan's Law\")\n",
    "ax.plot(sorted_V, sorted_N, c='b',marker=\"H\",linewidth=3., label=\"Empirical Corpus\")\n",
    "ax.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f70f16-1ec3-4ae8-a49a-e3aedf0ad0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"It has {len(nltk.FreqDist(w.lower() for w in brown.words()))} case-insensitive types\")\n",
    "print(f\"It has {len(nltk.FreqDist(w for w in brown.words()))} case-senstive types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa971e1a-e8c3-4278-9190-edae0ebe5b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"brown corpus has {len(brown.fileids())} files in total, it belongs to {len(brown.categories())} categories\")\n",
    "print(f\"first 10 file names: {brown.fileids()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff30cd64-787b-49da-bf4a-c36bacaa0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "fdist = len(nltk.FreqDist(w.lower() for w in news_text))\n",
    "fdist_case_sensitive = len(nltk.FreqDist(w for w in news_text))\n",
    "print(f\"there are {fdist} different words in news category!\")\n",
    "print(f\"there are {fdist_case_sensitive} case sensitive words in news category!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a51e06-3b4d-4930-a0be-987a4b5d769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"all categories of brown: {brown.categories()}\")\n",
    "print(f\"all words in news: {brown.words(categories='news')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d201eb84-e89c-439a-ae45-f365764c28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.words(fileids=['cg22']))\n",
    "print(brown.sents(categories=['news', 'editorial', 'reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654a55af-36e1-4c7a-80ac-29bbf1866f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_words = gutenberg.words('austen-emma.txt')\n",
    "type(emma_words)\n",
    "print(gutenberg.words('austen-emma.txt'))\n",
    "# How many tokens in the text:\n",
    "print(\"Token count:\", len(emma_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8115e-0800-4e2c-9cae-e830e5c78b9f",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Word Tokenization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09366a44-e4a8-498a-9647-db085cea5afb",
   "metadata": {},
   "source": [
    "There are two type of tokenizations\n",
    "\n",
    "- **Top-down tokenization**: We define a standard and implement rules to implement that kind of tokenization.\n",
    "  - word tokenization\n",
    "  - charater tokenization\n",
    "- **Bottom-up tokenization**: We use simple statistics of letter sequences to break up words into subword tokens.\n",
    "  - subword tokenization (modern LLMs use this type!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a899de-197e-4285-b5d7-3a3897cd270a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "text = \"\"\"ç”·ï¼šå¯¹ï¼Œä½ æ— æƒ…ï¼Œä½ æ®‹é…·ï¼Œä½ æ— ç†å–é—¹ï¼\n",
    "å¥³ï¼šé‚£ä½ å°±ä¸æ— æƒ…ï¼Œä¸æ®‹é…·ï¼Œä¸æ— ç†å–é—¹ï¼ï¼Ÿ\n",
    "ç”·ï¼šæˆ‘å“ªé‡Œæ— æƒ…ï¼ï¼Ÿå“ªé‡Œæ®‹é…·ï¼ï¼Ÿå“ªé‡Œæ— ç†å–é—¹ï¼ï¼Ÿ\n",
    "å¥³ï¼šä½ å“ªé‡Œä¸æ— æƒ…ï¼ï¼Ÿå“ªé‡Œä¸æ®‹é…·ï¼ï¼Ÿå“ªé‡Œä¸æ— ç†å–é—¹ï¼ï¼Ÿ\n",
    "ç”·ï¼šæˆ‘å°±ç®—å†æ€ä¹ˆæ— æƒ…ã€å†æ€ä¹ˆæ®‹é…·ã€å†æ€ä¹ˆæ— ç†å–é—¹ï¼Œä¹Ÿä¸ä¼šæ¯”ä½ æ›´æ— æƒ…ã€æ›´æ®‹é…·ã€æ›´æ— ç†å–é—¹ï¼\n",
    "å¥³ï¼šæˆ‘ä¼šæ¯”ä½ æ— æƒ…ï¼ï¼Ÿæ¯”ä½ æ®‹é…·ï¼ï¼Ÿæ¯”ä½ æ— ç†å–é—¹ï¼ï¼Ÿä½ æ‰æ˜¯æˆ‘è§è¿‡æœ€æ— æƒ…ã€æœ€æ®‹é…·ã€æœ€æ— ç†å–é—¹çš„äººã€‚\n",
    "ç”·ï¼šå“¼ï¼Œæˆ‘ç»å¯¹æ²¡ä½ æ— æƒ…ï¼Œæ²¡ä½ æ®‹é…·ï¼Œæ²¡ä½ æ— ç†å–é—¹ï¼\n",
    "å¥³ï¼šå¥½ï¼Œæ—¢ç„¶ä½ è¯´æˆ‘æ— æƒ…ï¼Œæˆ‘æ®‹é…·ï¼Œæˆ‘æ— ç†å–é—¹ï¼Œæˆ‘å°±æ— æƒ…ç»™ä½ çœ‹ï¼Œæ®‹é…·ç»™ä½ çœ‹ï¼Œæ— ç†å–é—¹ç»™ä½ çœ‹ã€‚\n",
    "ç”·ï¼šçœ‹å§ï¼Œè¿˜è¯´ä½ ä¸æ— æƒ…ï¼Œä¸æ®‹é…·ï¼Œä¸æ— ç†å–é—¹ã€‚ç°åœ¨å®Œå…¨å±•ç°ä½ æ— æƒ…ã€æ®‹é…·ã€æ— ç†å–é—¹çš„ä¸€é¢äº†å§ï¼Ÿï¼\"\"\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a5a64-8c57-47bd-8cd1-54d38c22c5b1",
   "metadata": {},
   "source": [
    "### Top-down (rule-based) tokenization - word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8327dd-356b-4e9d-a66e-2e5307d22b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::jieba --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813d1757-0e85-4d46-bbda-eb8495c7f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download zh_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb27dfb-ee31-4fc3-9932-a82576c65211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use split method via the whitespace \" \"\n",
    "text = \"\"\"While the Unix command sequence just removed all the numbers and punctuation\"\"\"\n",
    "print(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e319119-4e24-4513-8a04-f23648f44056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But, we have punctuations, icons, and many other small issues.\n",
    "text = \"\"\"Don't you love ğŸ¤— Transformers? We sure do.\"\"\"\n",
    "print(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d44cf-3dd9-45ec-bb66-3252d27edbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-down tokenization by using regular expression\n",
    "pattern = r'''(?x) # set flag to allow verbose regexps\n",
    "(?:[A-Z]\\.)+ # abbreviations, e.g. U.S.A. \n",
    "| \\w+(?:-\\w+)* # words with optional internal hyphens \n",
    "| \\$?\\d+(?:\\.\\d+)?%? # currency, percentages, e.g. $12.40, 82% \n",
    "| \\.\\.\\. # ellipsis \n",
    "| [][.,;\"'?():_`-] # these are separate tokens; includes ], [\n",
    "'''\n",
    "print(f'pattern needs to match is: \\n\\n{pattern}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459152ae-ac09-4d52-9fbe-41e0db067c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Don't you love ğŸ¤— Transformers? We sure do.\"\"\"\n",
    "print(f\"tokenized words after pattern matching: \\n\\n{nltk.regexp_tokenize(text, pattern)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8249de1e-b26e-46be-abe7-8b9750e69552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy works much better\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "for token in doc: \n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfdb70-28a3-49f7-832a-93eb8d487bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Special characters and numbers will need to be kept in prices ($45.55) and dates (01/02/06); \n",
    "we donâ€™t want to segment that price into separate tokens of â€œ45â€ and â€œ55â€. And there are URLs (https://www.stanford.edu),\n",
    "Twitter hashtags (#nlproc), or email addresses (someone@cs.colorado.edu).\"\"\"\n",
    "text = text.replace(\"\\n\", \" \").strip()\n",
    "print(f\"tokenized words after pattern matching: \\n\\n{nltk.regexp_tokenize(text, pattern)}\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "for token in doc: \n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be004a0f-d605-422e-a282-70c76010f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization is more complex in languages like written Chinese, Japanese.\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "text = 'å§šæ˜è¿›å…¥æ€»å†³èµ›'\n",
    "t = TreebankWordTokenizer()\n",
    "toks = t.tokenize(text)\n",
    "print('TreebankWordTokenizer: ', toks)\n",
    "\n",
    "# StanfordSegmenter for Chinese \n",
    "from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
    "# Note, it needs to install jar file.\n",
    "# Alternative way to tokenize Chinese words\n",
    "# install jieba via conda as: conda install conda-forge::jieba\n",
    "# Website: https://github.com/fxsjy/jieba\n",
    "import jieba\n",
    "\n",
    "text = 'å§šæ˜è¿›å…¥æ€»å†³èµ›'\n",
    "seg_list = jieba.cut(text)\n",
    "print('jieba package: ', \", \".join([_ for _ in seg_list]))\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "text = 'å§šæ˜è¿›å…¥æ€»å†³èµ›'\n",
    "doc = nlp(text)\n",
    "print('spacy package: ', ', '.join([str(token) for token in doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2770ef6-56aa-48eb-83f2-fbb2d6c55560",
   "metadata": {},
   "source": [
    "### Top-down (rule-based) tokenization - character tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba4145-db75-41cb-a30f-4b4f7709f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.zh import Chinese\n",
    "nlp_ch = Chinese()\n",
    "print(*nlp_ch(text), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0878f10-589f-447d-aab9-edd7b6d4be6d",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Word Tokenization: BPE</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaa316f-9c2a-4fda-8341-7e38dd55a85e",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding: A Bottom-up Tokenization Algorithm\n",
    "- It has been adopted from all modern LLMs including ChatGPT, GPT-series, and many others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a2cc19-90ae-4987-8601-bb7af4fe908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install conda-forge::tiktoken --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2b81e-fbcc-4ffe-8e84-8e572d501ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, install GPT-4's tiktoken via: conda install conda-forge::tiktoken\n",
    "import tiktoken\n",
    "# Load an encoding\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# Use tiktoken.encoding_for_model() to automatically load the correct encoding for a given model name.\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "print(encoding.encode(\"tiktoken is great!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da00af5-46d5-4248-b347-213bbf8f284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tokens by counting the length of the list returned by .encode().\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "text = \"tiktoken is great!\"\n",
    "print(f'\\\"{text}\\\" has been encoded into {num_tokens_from_string(text, \"cl100k_base\")} subwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213c7de-fd10-4a1c-8f34-ffb139302596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .decode() converts a list of token integers to a string.\n",
    "encode_ids = [83, 1609, 5963, 374, 2294, 0]\n",
    "print(f'the decoded string is: \\\"{encoding.decode(encode_ids)}\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dbfcea-b8e0-4607-9c0e-cc955756ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Chapters 5 to 8 teach the basics of ğŸ¤— Datasets and ğŸ¤— Tokenizers before diving into classic NLP tasks.\\\n",
    "By the end of this part, you will be able to tackle the most common NLP problems by yourself. \\\n",
    "By the end of this part, you will be ready to apply ğŸ¤— Transformers to (almost) any machine \\\n",
    "learning problem! E=mc^2. f(x) = x^2+y^2, print('hello world!â€™) baojianzhou. asdasfasdgasdg\n",
    "\"\"\"\n",
    "print(encoding.encode(text))\n",
    "\n",
    "encode_ids = encoding.encode(text)\n",
    "print(encoding.decode(encode_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5570dd-aae6-4d05-8df5-7832a2eff7fc",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Word Normalization, Lemmatization and Stemming</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da31918e-a69a-41c6-a3e5-c33542d2bfe3",
   "metadata": {},
   "source": [
    "### Lemmatization (è¯å½¢è¿˜åŸ)\n",
    "\n",
    "- Lemmatization is the task of determining that two words have the same root, despite their surface differences.\n",
    "- **Motivation**: For some NLP situations, we also want two morphologically different forms of a word to behave similarly. For example in web search, someone may type the string woodchucks but a useful system might want to also return pages\n",
    "that mention woodchuck with no s.\n",
    "- **Example 1**: The words am, are, and is have the shared lemma be.\n",
    "- **Example 2**: The words dinner and dinners both have the lemma dinner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a4e20-cdb3-45b3-bf2d-96c4ea838736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "text = \"\"\"\n",
    "The Brown Corpus, a text corpus of American English that was compiled in the 1960s at Brown University, \\\n",
    "is widely used in the field of linguistics and natural language processing. It contains about 1 million \\\n",
    "words (or \"tokens\") across a diverse range of texts from 500 sources, categorized into 15 genres, such \\\n",
    "as news, editorial, and fiction, to provide a comprehensive resource for studying the English language. \\\n",
    "This corpus has been instrumental in the development and evaluation of various computational linguistics \\\n",
    "algorithms and tools.\n",
    "\"\"\"\n",
    "text = text.replace(\"\\n\", \" \").strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e6507-946b-4022-b3fb-8d173357e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "print(doc[0], type(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d88cdb3-e2a8-4ec5-8ddf-c6696cddcf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [token.lemma_ for token in doc]\n",
    "for ori,lemma in zip(doc[:30], lemmas[:30]):\n",
    "    print(ori, lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a1363-5357-4fdf-a23c-54064c87be29",
   "metadata": {},
   "source": [
    "### Stemming (è¯å¹²æå–): The Porter-Stemmer method\n",
    "\n",
    "Lemmatization algorithms can be complex. For this reason we sometimes make use of a simpler but cruder method, which mainly consists of chopping off words final affixes. This naive version of morphological analysis is called stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f98b2-1aae-49d6-9483-45a1acc1d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy does not provide stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"\\\n",
    "This was not the map we found in Billy Bones's chest, but \\\n",
    "an accurate copy, complete in all things-names and heights \\\n",
    "and soundings-with the single exception of the red crosses \\\n",
    "and the written notes.\\\n",
    "\"\"\"   \n",
    "porter_stemmer = PorterStemmer()\n",
    "words = word_tokenize(text)\n",
    "for word in words:\n",
    "    print(word, porter_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc35830-ffbd-47be-8d6b-7c04947500c2",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Sentence Segmentation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e880c-295a-48c0-a1ef-d1a278784c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: use nltk package\n",
    "# Install nltk\n",
    "import nltk\n",
    "# Download the required models\n",
    "nltk.download('punkt')  \n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"In the first part of the book we introduce the fundamental suite of algorithmic \\\n",
    "tools that make up the modern neural language model that is the heart of end-to-end \\\n",
    "NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms \\\n",
    "like computing edit distance, and then proceed to the tasks of classification, \\\n",
    "logistic regression, neural networks, proceeding through feedforward networks, recurrent \\\n",
    "networks, and then transformers. Weâ€™ll also see the role of embeddings as a \\\n",
    "model of word meaning.\"\n",
    "sentences = sent_tokenize(text)\n",
    "for ind, sent in enumerate(sentences):\n",
    "    print(f\"sentence-{ind}: {sent}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5390e13-40d3-489d-8b79-d5f2c7ab28f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: A modern and fast NLP library that includes support for sentence segmentation. \n",
    "# spaCy uses a statistical model to predict sentence boundaries, which can be more accurate \n",
    "# than rule-based approaches for complex texts.\n",
    "# Install via conda: conda install conda-forge::spacy\n",
    "# Install via pip:   pip install -U spacy\n",
    "# Download data: python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Here is a sentence. Here is another one! And the last one.\")\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "for ind, sent in enumerate(sentences):\n",
    "    print(f\"sentence-{ind}: {sent}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a80c33-494d-40d4-99c9-1f84977d357e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # You need to install it via: python -m spacy download zh_core_web_sm\n",
    "from spacy.lang.zh.examples import sentences \n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "doc = nlp(sentences[0])\n",
    "text = \"\"\"\\\n",
    "å­—èŠ‚å¯¹ç¼–ç æ˜¯ä¸€ç§ç®€å•çš„æ•°æ®å‹ç¼©å½¢å¼ï¼Œè¿™ç§æ–¹æ³•ç”¨æ•°æ®ä¸­ä¸å­˜çš„ä¸€ä¸ªå­—èŠ‚è¡¨ç¤ºæœ€å¸¸å‡ºç°çš„è¿ç»­å­—èŠ‚æ•°æ®ã€‚è¿™æ ·çš„æ›¿æ¢éœ€è¦é‡å»ºå…¨éƒ¨åŸå§‹æ•°æ®ã€‚å­—èŠ‚å¯¹ç¼–ç å®ä¾‹: å‡è®¾æˆ‘ä»¬è¦ç¼–ç æ•°æ® aaabdaaabac, å­—èŠ‚å¯¹â€œaaâ€å‡ºç°æ¬¡æ•°æœ€å¤šï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨æ•°æ®ä¸­æ²¡æœ‰å‡ºç°çš„å­—èŠ‚â€œZâ€æ›¿æ¢â€œaaâ€å¾—åˆ°æ›¿æ¢è¡¨\n",
    "Z <- aa æ•°æ®è½¬å˜ä¸º ZabdZabac. åœ¨è¿™ä¸ªæ•°æ®ä¸­ï¼Œå­—èŠ‚å¯¹â€œZaâ€å‡ºç°çš„æ¬¡æ•°æœ€å¤šï¼Œæˆ‘ä»¬ç”¨å¦å¤–ä¸€ä¸ªå­—èŠ‚â€œYâ€æ¥æ›¿æ¢å®ƒï¼ˆè¿™ç§æƒ…å†µä¸‹ç”±äºæ‰€æœ‰çš„â€œZâ€éƒ½å°†è¢«æ›¿æ¢ï¼Œæ‰€ä»¥ä¹Ÿå¯ä»¥ç”¨â€œZâ€æ¥æ›¿æ¢â€œZaâ€ï¼‰ï¼Œå¾—åˆ°æ›¿æ¢è¡¨ä»¥åŠæ•°æ®\n",
    "Z <- aa, Y <- Za, YbdYbac. æˆ‘ä»¬å†æ¬¡æ›¿æ¢æœ€å¸¸å‡ºç°çš„å­—èŠ‚å¯¹å¾—åˆ°ï¼šZ <- aa, Y <- Za, X <- Yb. XdXac ç”±äºä¸å†æœ‰é‡å¤å‡ºç°çš„å­—èŠ‚å¯¹ï¼Œæ‰€ä»¥è¿™ä¸ªæ•°æ®ä¸èƒ½å†è¢«è¿›ä¸€æ­¥å‹ç¼©ã€‚è§£å‹çš„æ—¶å€™ï¼Œå°±æ˜¯æŒ‰ç…§ç›¸åçš„é¡ºåºæ‰§è¡Œæ›¿æ¢è¿‡ç¨‹ã€‚\n",
    "\"\"\"\n",
    "doc = nlp(text)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "for ind, sent in enumerate(sentences):\n",
    "    print(f\"sentence-{ind}: {sent}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08972d05-b262-4186-ab3f-effee06554b5",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\"> Minimum Edit Distance</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4951c71-002a-4550-983d-1139b7a8c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define minimum edit distance algorithm via dynamic programming\n",
    "def minimum_edit_distance(source, target):\n",
    "    n = len(source)\n",
    "    m = len(target)\n",
    "    d_mat = np.zeros((n + 1, m + 1))\n",
    "    for i in range(1, n + 1):\n",
    "        d_mat[i, 0] = i\n",
    "    for j in range(1, m + 1):\n",
    "        d_mat[0, j] = j\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            sub = 0 if source[i - 1] == target[j - 1] else 2\n",
    "            del_ = d_mat[i - 1][j] + 1\n",
    "            ins_ = d_mat[i][j - 1] + 1\n",
    "            d_mat[i][j] = min(del_, ins_, d_mat[i - 1][j - 1] + sub)\n",
    "    trace, align_source, align_target = backtrack_alignment(source, target, d_mat)\n",
    "    return d_mat[n, m], trace, align_source, align_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf8b60-862e-4f61-b32f-e829e9b8134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backtrack to identify actions of all minimum edits\n",
    "def backtrack_alignment(source, target, d_mat):\n",
    "    align_source, align_target = [], []\n",
    "    i = len(source)\n",
    "    j = len(target)\n",
    "    back_trace = [[i, j]]\n",
    "    while (i, j) != (0, 0):\n",
    "        sub = 0 if source[i - 1] == target[j - 1] else 2\n",
    "        del_ = d_mat[i - 1][j]\n",
    "        ins_ = d_mat[i][j - 1]\n",
    "        # substitution operation\n",
    "        if d_mat[i][j] == d_mat[i - 1][j - 1] + sub:\n",
    "            back_trace.append([i - 1, j - 1])\n",
    "            align_source = [source[i - 1]] + align_source\n",
    "            align_target = [target[j - 1]] + align_target\n",
    "            i, j = i - 1, j - 1\n",
    "        else:\n",
    "            # deletion operation\n",
    "            if d_mat[i][j] == del_ + 1:\n",
    "                back_trace.append([i - 1, j])\n",
    "                align_source = [source[i - 1]] + align_source\n",
    "                align_target = [\"*\"] + align_target\n",
    "                i, j = i - 1, j\n",
    "            # insertion operation\n",
    "            elif d_mat[i][j] == ins_ + 1:\n",
    "                back_trace.append([i, j - 1])\n",
    "                align_source = [\"*\"] + align_source\n",
    "                align_target = [target[j - 1]] + align_target\n",
    "                i, j = i, j - 1\n",
    "    return back_trace, align_source, align_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc881802-b0a3-494a-99a6-ab2d19b9e365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the minimum edit distance\n",
    "def test_med(source, target):\n",
    "    med, trace, align_source, align_target = minimum_edit_distance(source, target)\n",
    "    print(f\"input source: {source} and target: {target}\")\n",
    "    print(f\"med: {med}\")\n",
    "    print(f\"trace: {trace}\")\n",
    "    print(f\"aligned source: {align_source}\")\n",
    "    print(f\"aligned target: {align_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad0205-bc60-4275-8a4e-30786d70b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_med(source=\"INTENTION\", target=\"EXECUTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed32704-a33c-4bf3-808b-2b0afba791b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_med(source=\"AGGCTATCACCTGACCTCCAGGCCGATGCCC\", target=\"TAGCTATCACGACCGCGGTCGATTTGCCCGAC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444aa0de-e269-4ba3-8991-ad8b0aeba222",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Other useful tutorials</h2>\n",
    "\n",
    "- Alice Zhao's NLP with Python: https://www.youtube.com/watch?v=xvqsFTUsOmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def1a63-08f9-4a09-88f1-b58622ef6fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
